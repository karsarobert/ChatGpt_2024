{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYr2hNSHiNmy2gHvA7mupl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karsarobert/ChatGpt_2024/blob/main/09/ChatGPT2024_09_FELADATOKKAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chat GPT és más nagy nyelvi modellek alkalmazása\n",
        "##PTE Gépi tanulás\n",
        "###9. gyakorlat: Langchain 1. rész\n",
        "2024. április 8."
      ],
      "metadata": {
        "id": "PgDtxlPV3A9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A jegyzetfüzethez kell OpenAI és Huggingface API kulcs!"
      ],
      "metadata": {
        "id": "5eNjqSz_8ceJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai tiktoken faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "id": "oEdX6JS4nJp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modellek és promptok\n",
        "A LangChain több mint 50 integrációt kínál külső gyártók és platformok felé, beleértve az OpenAI-t, az Azure OpenAI-t, a Databricks-et és a MosaicML-t, valamint az integrációt a Hugging Face Hub-bal és a nyílt forráskódú LLM-ek világával."
      ],
      "metadata": {
        "id": "QuC_1ltVvOUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "YuYe2VE4x6If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(api_key = userdata.get('openai_api_key'))\n",
        "print(llm('tell me a joke'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s65yw9JStLdx",
        "outputId": "cadfc90d-f366-4ff1-d05f-ddb03a32d70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Why did the tomato turn red?\n",
            "\n",
            "Because it saw the salad dressing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt sablonok.\n",
        "A prompt sablon egy olyan elem, ami meghatározza, hogyan lehet létrehozni egy utasítást (promptot) egy nyelvi modell számára. Tartalmazhat változókat, helyőrzőket, elő- és utótagokat, illetve más elemeket, amelyek az adatoknak és a feladatnak megfelelően testreszabhatók."
      ],
      "metadata": {
        "id": "h0_I-LJowQDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "template = \"\"\"Sentence: {sentence}\n",
        "Translation in {language}:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
        "print(prompt.format(sentence = \"the cat is on the table\", language = \"hungarian\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WKt4Vh9tpuH",
        "outputId": "83a60861-2d85-4403-e147-6f0850908339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: the cat is on the table\n",
            "Translation in hungarian:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Általánosságban elmondható, hogy a prompt sablonok jellemzően függetlenek attól, hogy végül melyik LLM-et használjuk, és mind a befejezési (completion), mind a chat modellekhez alkalmazhatóak.\n",
        "\n",
        "Befejezési modell (completion model): Ez egy olyan LLM-típus, amely szöveges inputot kap, és szöveges outputot generál (ezt nevezzük befejezésnek). A befejezési modell megpróbálja koherens és a témához illő módon folytatni a promptot, annak megfelelően, hogy milyen feladatra és milyen adatokra képezték ki. Például egy befejezési modell generálhat összefoglalásokat, fordításokat, történeteket, kódot, dalszöveget és még sok mást a prompttól függően.\n",
        "\n",
        "Chat modell: A befejezési modell egy speciális típusa, amelyet beszélgetős válaszok generálására terveztek. A chat modell bemenetként egy üzenetlistát kap, ahol minden üzenet rendelkezik egy szereppel (rendszer, felhasználó vagy asszisztens) és tartalommal. A chat modell megpróbál új üzenetet generálni az asszisztens szerepéhez, a korábbi üzenetek és a rendszer utasítása alapján.\n",
        "\n",
        "A fő különbség a befejezési és a chat modellek között az, hogy a befejezési modellek egyetlen szöveges bemenetet várnak promptként, míg a chat modellek bemenetként üzenetek listáját.\n",
        "\n",
        "Példaválasztó (Example selector). A LangChainben a példa kiválasztó egy olyan komponens, amellyel kiválaszthatod, mely példákat illeszd be a nyelvi modell promptjába. A prompt egy olyan szöveges bemenet, amely a nyelvi modellt a kívánt kimenet előállítására irányítja. A példák olyan bemeneti/kimeneti párok, amelyek bemutatják a feladatot és a kimenet formátumát."
      ],
      "metadata": {
        "id": "yL7x9QTQw2xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hgaJhmWIwUbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "# Define the ticker symbols\n",
        "tickers = ['AAPL']\n",
        "# Define the start and end dates\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2022-12-31'\n",
        "# Create an empty DataFrame to store the data\n",
        "data = pd.DataFrame()\n",
        "# Download the data\n",
        "for ticker in tickers:\n",
        " df = yf.download(ticker, start=start_date, end=end_date, interval='1mo')\n",
        " df['Ticker'] = ticker # Add a column with the ticker symbol\n",
        " data = pd.concat([data, df])\n",
        "# Reset the index\n",
        "data.reset_index(inplace=True)\n",
        "# Save the data to a CSV file\n",
        "data.to_csv('stock_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpB7HX1Bumqu",
        "outputId": "761584eb-030e-4a60-c814-99f2bd28a785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dokumentum betöltők (Document loaders).\n",
        "Ezek felelősek a dokumentumok betöltéséért különböző forrásokból, mint például CSV, fájlkönyvtárak (File Directory), HTML, JSON, Markdown és PDF. A dokumentum betöltők egy .load metódust biztosítanak, amellyel adatokat tölthetsz be dokumentumokként egy konfigurált forrásból. A kimenet egy Document objektum, ami egy szövegrészletet és a hozzá tartozó metaadatokat tartalmazza."
      ],
      "metadata": {
        "id": "ug82H4f_xcL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "loader = CSVLoader(file_path='stock_data.csv')\n",
        "data = loader.load()\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sC6w3byuOK3",
        "outputId": "68b1e19c-6553-47d5-b002-dde9b0a57656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Date: 2020-01-01\\nOpen: 74.05999755859375\\nHigh: 81.9625015258789\\nLow: 73.1875\\nClose: 77.37750244140625\\nAdj Close: 75.2875747680664\\nVolume: 2934370400\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 0}), Document(page_content='Date: 2020-02-01\\nOpen: 76.07499694824219\\nHigh: 81.80500030517578\\nLow: 64.09249877929688\\nClose: 68.33999633789062\\nAdj Close: 66.49418640136719\\nVolume: 3019279200\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 1}), Document(page_content='Date: 2020-03-01\\nOpen: 70.56999969482422\\nHigh: 76.0\\nLow: 53.15250015258789\\nClose: 63.5724983215332\\nAdj Close: 62.00223922729492\\nVolume: 6280072400\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 2}), Document(page_content='Date: 2020-04-01\\nOpen: 61.625\\nHigh: 73.63249969482422\\nLow: 59.224998474121094\\nClose: 73.44999694824219\\nAdj Close: 71.63575744628906\\nVolume: 3265299200\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 3}), Document(page_content='Date: 2020-05-01\\nOpen: 71.5625\\nHigh: 81.05999755859375\\nLow: 71.4625015258789\\nClose: 79.48500061035156\\nAdj Close: 77.52169799804688\\nVolume: 2805936000\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 4}), Document(page_content='Date: 2020-06-01\\nOpen: 79.4375\\nHigh: 93.09500122070312\\nLow: 79.30249786376953\\nClose: 91.19999694824219\\nAdj Close: 89.1881103515625\\nVolume: 3243375600\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 5}), Document(page_content='Date: 2020-07-01\\nOpen: 91.27999877929688\\nHigh: 106.41500091552734\\nLow: 89.1449966430664\\nClose: 106.26000213623047\\nAdj Close: 103.91588592529297\\nVolume: 3020283200\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 6}), Document(page_content='Date: 2020-08-01\\nOpen: 108.19999694824219\\nHigh: 131.0\\nLow: 107.89250183105469\\nClose: 129.0399932861328\\nAdj Close: 126.1933364868164\\nVolume: 4070061100\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 7}), Document(page_content='Date: 2020-09-01\\nOpen: 132.75999450683594\\nHigh: 137.97999572753906\\nLow: 103.0999984741211\\nClose: 115.80999755859375\\nAdj Close: 113.45938873291016\\nVolume: 3885245100\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 8}), Document(page_content='Date: 2020-10-01\\nOpen: 117.63999938964844\\nHigh: 125.38999938964844\\nLow: 107.72000122070312\\nClose: 108.86000061035156\\nAdj Close: 106.65047454833984\\nVolume: 2894666500\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 9}), Document(page_content='Date: 2020-11-01\\nOpen: 109.11000061035156\\nHigh: 121.98999786376953\\nLow: 107.31999969482422\\nClose: 119.05000305175781\\nAdj Close: 116.6336441040039\\nVolume: 2123077300\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 10}), Document(page_content='Date: 2020-12-01\\nOpen: 121.01000213623047\\nHigh: 138.7899932861328\\nLow: 120.01000213623047\\nClose: 132.69000244140625\\nAdj Close: 130.22108459472656\\nVolume: 2322189600\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 11}), Document(page_content='Date: 2021-01-01\\nOpen: 133.52000427246094\\nHigh: 145.08999633789062\\nLow: 126.37999725341797\\nClose: 131.9600067138672\\nAdj Close: 129.5046844482422\\nVolume: 2240262000\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 12}), Document(page_content='Date: 2021-02-01\\nOpen: 133.75\\nHigh: 137.8800048828125\\nLow: 118.38999938964844\\nClose: 121.26000213623047\\nAdj Close: 119.00373840332031\\nVolume: 1833855600\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 13}), Document(page_content='Date: 2021-03-01\\nOpen: 123.75\\nHigh: 128.72000122070312\\nLow: 116.20999908447266\\nClose: 122.1500015258789\\nAdj Close: 120.05631256103516\\nVolume: 2650418200\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 14}), Document(page_content='Date: 2021-04-01\\nOpen: 123.66000366210938\\nHigh: 137.07000732421875\\nLow: 122.48999786376953\\nClose: 131.4600067138672\\nAdj Close: 129.2067413330078\\nVolume: 1889857500\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 15}), Document(page_content='Date: 2021-05-01\\nOpen: 132.0399932861328\\nHigh: 134.07000732421875\\nLow: 122.25\\nClose: 124.61000061035156\\nAdj Close: 122.47415924072266\\nVolume: 1711934900\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 16}), Document(page_content='Date: 2021-06-01\\nOpen: 125.08000183105469\\nHigh: 137.41000366210938\\nLow: 123.12999725341797\\nClose: 136.9600067138672\\nAdj Close: 134.8411102294922\\nVolume: 1606590000\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 17}), Document(page_content='Date: 2021-07-01\\nOpen: 136.60000610351562\\nHigh: 150.0\\nLow: 135.75999450683594\\nClose: 145.86000061035156\\nAdj Close: 143.60342407226562\\nVolume: 1919035100\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 18}), Document(page_content='Date: 2021-08-01\\nOpen: 146.36000061035156\\nHigh: 153.49000549316406\\nLow: 144.5\\nClose: 151.8300018310547\\nAdj Close: 149.48106384277344\\nVolume: 1461542800\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 19}), Document(page_content='Date: 2021-09-01\\nOpen: 152.8300018310547\\nHigh: 157.25999450683594\\nLow: 141.27000427246094\\nClose: 141.5\\nAdj Close: 139.51959228515625\\nVolume: 1797835100\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 20}), Document(page_content='Date: 2021-10-01\\nOpen: 141.89999389648438\\nHigh: 153.1699981689453\\nLow: 138.27000427246094\\nClose: 149.8000030517578\\nAdj Close: 147.70346069335938\\nVolume: 1565079200\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 21}), Document(page_content='Date: 2021-11-01\\nOpen: 148.99000549316406\\nHigh: 165.6999969482422\\nLow: 147.47999572753906\\nClose: 165.3000030517578\\nAdj Close: 162.98651123046875\\nVolume: 1691029000\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 22}), Document(page_content='Date: 2021-12-01\\nOpen: 167.47999572753906\\nHigh: 182.1300048828125\\nLow: 157.8000030517578\\nClose: 177.57000732421875\\nAdj Close: 175.34031677246094\\nVolume: 2444766700\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 23}), Document(page_content='Date: 2022-01-01\\nOpen: 177.8300018310547\\nHigh: 182.94000244140625\\nLow: 154.6999969482422\\nClose: 174.77999877929688\\nAdj Close: 172.5853271484375\\nVolume: 2108446000\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 24}), Document(page_content='Date: 2022-02-01\\nOpen: 174.00999450683594\\nHigh: 176.64999389648438\\nLow: 152.0\\nClose: 165.1199951171875\\nAdj Close: 163.04664611816406\\nVolume: 1627516300\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 25}), Document(page_content='Date: 2022-03-01\\nOpen: 164.6999969482422\\nHigh: 179.61000061035156\\nLow: 150.10000610351562\\nClose: 174.61000061035156\\nAdj Close: 172.63714599609375\\nVolume: 2180800100\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 26}), Document(page_content='Date: 2022-04-01\\nOpen: 174.02999877929688\\nHigh: 178.49000549316406\\nLow: 155.3800048828125\\nClose: 157.64999389648438\\nAdj Close: 155.86875915527344\\nVolume: 1687795600\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 27}), Document(page_content='Date: 2022-05-01\\nOpen: 156.7100067138672\\nHigh: 166.47999572753906\\nLow: 132.61000061035156\\nClose: 148.83999633789062\\nAdj Close: 147.15829467773438\\nVolume: 2401040300\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 28}), Document(page_content='Date: 2022-06-01\\nOpen: 149.89999389648438\\nHigh: 151.74000549316406\\nLow: 129.0399932861328\\nClose: 136.72000122070312\\nAdj Close: 135.37387084960938\\nVolume: 1749099800\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 29}), Document(page_content='Date: 2022-07-01\\nOpen: 136.0399932861328\\nHigh: 163.6300048828125\\nLow: 135.66000366210938\\nClose: 162.50999450683594\\nAdj Close: 160.90992736816406\\nVolume: 1447125400\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 30}), Document(page_content='Date: 2022-08-01\\nOpen: 161.00999450683594\\nHigh: 176.14999389648438\\nLow: 157.13999938964844\\nClose: 157.22000122070312\\nAdj Close: 155.6719970703125\\nVolume: 1510239600\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 31}), Document(page_content='Date: 2022-09-01\\nOpen: 156.63999938964844\\nHigh: 164.25999450683594\\nLow: 138.0\\nClose: 138.1999969482422\\nAdj Close: 137.0293731689453\\nVolume: 2084722800\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 32}), Document(page_content='Date: 2022-10-01\\nOpen: 138.2100067138672\\nHigh: 157.5\\nLow: 134.3699951171875\\nClose: 153.33999633789062\\nAdj Close: 152.04112243652344\\nVolume: 1868139700\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 33}), Document(page_content='Date: 2022-11-01\\nOpen: 155.0800018310547\\nHigh: 155.4499969482422\\nLow: 134.3800048828125\\nClose: 148.02999877929688\\nAdj Close: 146.77610778808594\\nVolume: 1724847700\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 34}), Document(page_content='Date: 2022-12-01\\nOpen: 148.2100067138672\\nHigh: 150.9199981689453\\nLow: 125.87000274658203\\nClose: 129.92999267578125\\nAdj Close: 129.04312133789062\\nVolume: 1675731200\\nTicker: AAPL', metadata={'source': 'stock_data.csv', 'row': 35})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dokumentum átalakítók (Document transformers).\n",
        "A dokumentumok importálása után gyakori, hogy módosítani kell őket, hogy jobban megfeleljenek az igényeidnek. Erre egy alapvető példa, amikor egy hosszú dokumentumot kisebb részekre bontasz, amelyek illeszkednek a modelled kontextusablakába (a modell által egyszerre feldolgozható szövegmennyiségbe). A LangChain-en belül különböző, előre elkészített dokumentum átalakítók állnak rendelkezésre, amelyeket szövegfelosztóknak (Text splitters) neveznek. A szövegfelosztók célja, hogy megkönnyítsék a dokumentumok szemantikailag kapcsolódó részekre bontását, így nem veszítünk kontextust vagy releváns információt.\n",
        "\n",
        "A szövegfelosztókkal eldöntheted, hogyan oszd fel a szöveget (pl. karakter, címsor, token stb. alapján), és hogyan mérd meg a szövegrészlet hosszát (pl. a karakterek száma alapján)."
      ],
      "metadata": {
        "id": "I9NDaCa3xzJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 183bdW5i6KCv5fq_nZGx3nOIi6N70gZ0x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYwEaV49yQzP",
        "outputId": "b05fc970-b99d-463e-f0b5-ffb941296547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=183bdW5i6KCv5fq_nZGx3nOIi6N70gZ0x\n",
            "To: /content/piszkosfred_reszlet.txt\n",
            "\r  0% 0.00/5.22k [00:00<?, ?B/s]\r100% 5.22k/5.22k [00:00<00:00, 8.99MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"piszkosfred_reszlet.txt\")\n",
        "text = loader.load()\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size = 200,\n",
        "  chunk_overlap = 50,\n",
        "  length_function = len\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(text)\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP3Apw9WzY2M",
        "outputId": "8af30850-af80-49e5-9041-edee6eacd90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='\\ufeff\\nRejtő Jenő\\n\\nPiszkos Fred, a kapitány' metadata={'source': 'piszkosfred_reszlet.txt'}\n",
            "page_content='ELSŐ FEJEZET\\n1\\n- Uram! A késemért jöttem!\\n- Hol hagyta?\\n- Valami matrózban.\\n- Milyen kés volt?\\n- Acél. Keskeny penge, kissé hajlott. Nem látta?' metadata={'source': 'piszkosfred_reszlet.txt'}\n",
            "page_content='- Acél. Keskeny penge, kissé hajlott. Nem látta?\\n- Várjunk... Csak lassan, kérem... Milyen volt a nyele?\\n- Kagyló.\\n- Hány részből?\\n- Egy darabból készült.\\n- Akkor nincs baj. Megvan a kés!\\n- Hol?' metadata={'source': 'piszkosfred_reszlet.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('piszkosfred_reszlet.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size = 200,\n",
        "  chunk_overlap = 50,\n",
        "  length_function = len\n",
        ")\n",
        "texts = text_splitter.create_documents([text])\n",
        "print(texts[0])\n",
        "print(texts[1])\n",
        "print(texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNHiiWmPu1nl",
        "outputId": "4465ff90-d6a5-4805-991c-69063de945d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='\\ufeff\\nRejtő Jenő\\n\\nPiszkos Fred, a kapitány'\n",
            "page_content='ELSŐ FEJEZET\\n1\\n- Uram! A késemért jöttem!\\n- Hol hagyta?\\n- Valami matrózban.\\n- Milyen kés volt?\\n- Acél. Keskeny penge, kissé hajlott. Nem látta?'\n",
            "page_content='- Acél. Keskeny penge, kissé hajlott. Nem látta?\\n- Várjunk... Csak lassan, kérem... Milyen volt a nyele?\\n- Kagyló.\\n- Hány részből?\\n- Egy darabból készült.\\n- Akkor nincs baj. Megvan a kés!\\n- Hol?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Szövegbeágyazó modellek (Text embedding models).\n",
        "A beágyazás a szavak, részek vagy karakterek folyamatos vektortérben történő ábrázolásának módját jelenti.\n",
        "\n",
        "A beágyazások kulcsfontosságú lépést jelentenek a nem-parametrikus tudás beépítésében az LLM-ekbe. Valójában, miután megfelelően tárolták őket egy VectorDB-ben, válnak azzá a nem-parametrikus tudássá, amellyel szemben mérni tudjuk a felhasználó lekérdezésének távolságát.\n",
        "\n",
        "A beágyazás használatának megkezdéséhez szükség lesz egy beágyazási modellre:\n",
        "\n",
        "Ezután a LangChain két fő modullal rendelkező Embedding osztályt kínál, amelyek a nem-parametrikus tudás (több bemeneti szöveg), illetve a felhasználói lekérdezés (egyetlen bemeneti szöveg) beágyazását célozzák.\n",
        "\n",
        "Vegyünk például egy beágyazást az OpenAI text-embedding-ada-002 beágyazási modelljével (az OpenAI beágyazási modelljeivel kapcsolatos további részletekért az hivatalos dokumentációt a következő címen érheted el: https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)"
      ],
      "metadata": {
        "id": "7pKv0C8Y4IP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings(model ='text-embedding-ada-002' ,api_key = userdata.get('openai_api_key'))\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        \"Good morning!\",\n",
        "        \"Oh, hello!\",\n",
        "        \"I want to report an accident\",\n",
        "        \"Sorry to hear that. May I ask your name?\",\n",
        "        \"Sure, Mario Rossi.\"\n",
        "    ]\n",
        ")\n",
        "print(\"Embed documents:\")\n",
        "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}\")\n",
        "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
        "print(\"Embed query:\")\n",
        "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
        "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOwjepHBvVVc",
        "outputId": "d9a69d48-d280-441f-889e-bfceee61c8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embed documents:\n",
            "Number of vector: 5; Dimension of each vector: 1536\n",
            "Embed query:\n",
            "Dimension of the vector: 1536\n",
            "Sample of the first 5 elements of the vector: [0.0053848074247278025, -0.0005522561790177142, 0.038960665101309515, -0.0029398672940039064, -0.008987877434176594]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings_modelHF = HuggingFaceEmbeddings()\n",
        "\n",
        "embeddingsHF = embeddings_modelHF.embed_documents(\n",
        "    [\n",
        "        \"Good morning!\",\n",
        "        \"Oh, hello!\",\n",
        "        \"I want to report an accident\",\n",
        "        \"Sorry to hear that. May I ask your name?\",\n",
        "        \"Sure, Mario Rossi.\"\n",
        "    ]\n",
        ")\n",
        "print(\"Embed documents:\")\n",
        "print(f\"Number of vector: {len(embeddingsHF)}; Dimension of each vector: {len(embeddingsHF[0])}\")\n",
        "embedded_query = embeddings_modelHF.embed_query(\"What was the name mentioned in the conversation?\")\n",
        "print(\"Embed query:\")\n",
        "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
        "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476,
          "referenced_widgets": [
            "13d307d871d24ca98f660f7b6032ca24",
            "bb29f2f8ed9d4d9991d320dac2ec7f25",
            "90b2d137af8848dcb187f951c0d73ea9",
            "f8c5b3969c284afc97a3f94f1be42190",
            "736b4d20bbc442a9b4812e55983c2684",
            "3ffa22a4bc7b4097a0222970b4bf4521",
            "61fa0ed761c24594af40e8ae1066f532",
            "12241e8b27b2477789369d1afd3870aa",
            "1e693ae9fccc4a3c9211482f9ec15e1f",
            "28a39517214a46c5b2941eb81acf64ba",
            "12c54960451746e5bd0f5c800253c45d",
            "ee938a22be14411c8b4a13311ad53cec",
            "66141423b0614e68b613090b466d4083",
            "19bc80f0ef764a2baa4a7b30b107db74",
            "74db321d81b44477b157328d3206f483",
            "e9c03da66f1e49a89a26a3167df3311d",
            "82be46d8937c4ac5af46ed58019b2355",
            "e9a17f9ce0f54cf18cd52925bbe1238f",
            "f2358afb8b3243e790ff7c7ac3449bb7",
            "5505f8459d624d5680b85be3f052b569",
            "8daaae8765a3431b944d1b711c7b6562",
            "348ea0d2f06245b9b0828dd4b1e67124",
            "b8fcdeec6f7944d3b102cf160d7c909b",
            "404fb5539c5445e8b3473c1cc3785d1b",
            "4a7ff0bfe02d4600b64254af15deab4e",
            "442e272bba06409fbf9c927e9c756d4e",
            "91ac34b3ccdb44c79ccb8d859bc424f2",
            "a066dee3adc74af99a9fc10b1000d92b",
            "1afe255d9a334b85bc549f09560e67a2",
            "68bf2b94737043a3af50f67497ef0a15",
            "ed7ff87316184499bc085a0c3100d36c",
            "d414ad6ff8c84da2a4b11c2b00f2d403",
            "c9bde135b6e5494e8e7752397f45adb8",
            "e037114d4f8f45008927b690fdc85a8e",
            "7ae7d21be3d146e699f9c189060195eb",
            "dba46579005147ec998617a1aff4ba17",
            "703172de8ce64085aee441b70cbb5751",
            "84a2417fc1bb40ef940ee3279a7d5a03",
            "62b1f130b698499b93d6cb62101312d5",
            "b881ffcc3b874ee4883c5cd80bba3ee9",
            "27d175b7d6374bb18f8bedb6714c190c",
            "14226105745845b7a5a16aba491ef815",
            "95d2842eb6fb4657bfd34333e48aa1fc",
            "fb49ba1d87ee4a769bf37ef964720edf",
            "52cd0e6607b543d2b7d2aae781730271",
            "31e31d99a86f4a0a8b844de4fa50e6b1",
            "931d90b4ed344edca31c456f3dd9aa10",
            "04f1959e2cd24cd3844d006253ccfb96",
            "0abdc623da6e49e79f92ab6a0a35df2d",
            "7dc6895c6c5b4c3a86b613d847bc0f42",
            "17146afadc82480d8d3542dbfed11061",
            "cbca68abfb1040db9207b49fd520efef",
            "d4c27447c90a42dc8aca1a7823b28f93",
            "fa8eaa7a7431468eaaeebf217dec744b",
            "79f044efc5f34ceab106bd4639c0116d",
            "955c82037c564342986abdc8a1c39d27",
            "6b23c51dfa4349c1b830464048c47b6a",
            "2c7fc84881394b04b523565a44bb4436",
            "04c115bd4c97496c93462f6b1b89e0db",
            "149d4062ab9043d18666ac8e1009354f",
            "5d8603081cf24012b1198c20d7a5e4a4",
            "d88caecb14f64233a6f1c78bf64393c7",
            "7563fbb2a7b24b9190980413c7ea2bc4",
            "8a7767cefdee4583913d3b651105f00a",
            "2a272fecb12c4f19b0b308d30f1d6414",
            "eea60d6ae4434db9a6bd7ba6877b8af1",
            "1fd376422c8a4debbb38ed80375b8bed",
            "2307f29d1330461b93e9ee18d93c78b0",
            "9310546e2e7948c5b7d63ef1b7b49c6f",
            "2ead4f36e9da4b788b4c658f03779615",
            "d831491a4d484d0fb76ef03620ca1a04",
            "b6cf894b641e4438adbc88beaf71d48c",
            "1ce48a63dfcb41d29946bec47f1829b0",
            "d68e988dac6041eebcb142097e54b892",
            "21a55782058e4452921f6ffec5ed7f23",
            "981eab70809e47558597bbec98e5dd38",
            "edb8aa66eb5b4ea982c03973d3661037",
            "76b87b4f4c744822b11e3749edb6d0d4",
            "9bca36f7868946aaa655c94fee9d2be4",
            "bfa6bb0d4af94d96a4e828ed02ef63a9",
            "dcbc1bdca4ec41f4830253975830b39c",
            "f227e494ba6148799caa9b8234916a1a",
            "8420b47e8d224835ad4ba8af1288b597",
            "bc594794ca06410e92bb00b5403008b3",
            "8bdd603e738f4c28874441845a64e5e8",
            "fe9861b3641b48b69f0798e59b478fc0",
            "6a69e710385348bba6f2f674dd6e326e",
            "34116f1863a7491d86476e42b4973c4d",
            "51d7157505d94f25a7edf475ab1a12cb",
            "9f76f3cd3c7e46b1a6c740326e0ae8d6",
            "f600a128add5478ca55af33a2c811e96",
            "5006a953465e4edfabca9153f1435a3e",
            "0472e3b31e484c1d96406fbfe04232ff",
            "a5067cd1db4849e68257bfde809b1540",
            "3c03a95e0a004b27816f10039b9a5cf5",
            "4a52df7d38124ad38c0ed360557b387f",
            "17db233db0cc40b7a71109b5b570438c",
            "ad98781869c34baf945efa8b43e09d6c",
            "6efb9155963149c5b988f9918dc8fecd",
            "d2a9415ec3e74f4bb678b207903ae348",
            "2d5807ad8ff34bf08dc7841075ecef95",
            "b05da2928be54cdb8cbf80cdbed055ef",
            "83522b0d90454adda672334b991e9d8d",
            "d4857826ee204951ab585c6f38c247ab",
            "8e9c76e5096a4e5e9f27997f53642702",
            "8c40683ef01747178f327e7e43dd1456",
            "f53786d7901a40e297226656d9d3a2da",
            "17b05b0b70b44efcbaa231577ae90a9b",
            "c5fab6f699974a7b8ebcaf8ebd6361ec",
            "3e9179861e944051be88875504dea2c1",
            "074258b73edb4dbc8fa9969c6106b58c",
            "38781d4fd3f548b48cf2098a0e81023b",
            "ce1c29258af04a4a873c32fed16af352",
            "ef506ec8bb6c4416b77c2ebfc4eb67de",
            "4ea90eab3c1b441084128714337787c3",
            "739568a3ae5f4476be59f4025b6d804b",
            "311fd76ec40047dfb4866807ce9ccdc8",
            "93a1ee69cc75496b85f48787cdf9bf0e",
            "12c785a3a28b47b98de361d060bce37f",
            "777dcf574b3a4d72bf3fc4fc3720d0ff",
            "575614161a9443a3bea8cc39aba68a1c"
          ]
        },
        "id": "DxNICjTu5zrw",
        "outputId": "1a2482a0-7487-42c7-c964-17e02c1094b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13d307d871d24ca98f660f7b6032ca24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee938a22be14411c8b4a13311ad53cec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8fcdeec6f7944d3b102cf160d7c909b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e037114d4f8f45008927b690fdc85a8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52cd0e6607b543d2b7d2aae781730271"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "955c82037c564342986abdc8a1c39d27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fd376422c8a4debbb38ed80375b8bed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76b87b4f4c744822b11e3749edb6d0d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51d7157505d94f25a7edf475ab1a12cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2a9415ec3e74f4bb678b207903ae348"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "074258b73edb4dbc8fa9969c6106b58c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embed documents:\n",
            "Number of vector: 5; Dimension of each vector: 768\n",
            "Embed query:\n",
            "Dimension of the vector: 768\n",
            "Sample of the first 5 elements of the vector: [0.09514584392309189, 9.877218690235168e-05, -0.016573388129472733, 0.044848017394542694, 0.04323701933026314]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Miután beágyaztuk mind a dokumentumokat, mind a lekérdezést, a következő lépés a két elem közötti hasonlóság kiszámítása, és a legmegfelelőbb információ kinyerése a dokumentumok beágyazásából.\n",
        "\n",
        "Vektortárolók (Vector stores). A vektortároló (vagy VectorDB) egy olyan adattbázis-típus, amely strukturálatlan adatokat, például szöveget, képeket, hangot vagy videót tud tárolni és azokon keresni beágyazások (embedding-ek) segítségével. A beágyazások használatával a vektortárolók gyors és pontos hasonlósági keresést tudnak végrehajtani, ami azt jelenti, hogy megtalálják a legrelevánsabb adatokat egy adott lekérdezéshez."
      ],
      "metadata": {
        "id": "Gb5Zf8Hr8G6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1tICd5l-gv1PhZgvCkLD4ydqq-jIaHUv-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIfEkV4L8oZS",
        "outputId": "94e0ce35-f00e-4de8-83d7-35db1e4ece17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tICd5l-gv1PhZgvCkLD4ydqq-jIaHUv-\n",
            "To: /content/dialog.txt\n",
            "\r  0% 0.00/114 [00:00<?, ?B/s]\r100% 114/114 [00:00<00:00, 369kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OpenAi beágyazásokkal"
      ],
      "metadata": {
        "id": "gx3WTZPZAcNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
        "raw_documents = TextLoader('dialog.txt').load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "db = FAISS.from_documents(documents, OpenAIEmbeddings(api_key = userdata.get('openai_api_key')))"
      ],
      "metadata": {
        "id": "zgsL28_BwP01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the reason for calling?\"\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8974b3b-6e23-4852-8b44-772e12776e04",
        "id": "YDwqOvPt911h"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to report an accident\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Huggingface alapmodell beágyazásokkal"
      ],
      "metadata": {
        "id": "tWSlJ8BUAiWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embeddings_modelHF = HuggingFaceEmbeddings()\n",
        "\n",
        "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
        "raw_documents = TextLoader('dialog.txt').load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "dbHF = FAISS.from_documents(documents, embeddings_modelHF)"
      ],
      "metadata": {
        "id": "6ztXxV6G9F1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the reason for calling?\"\n",
        "docs = dbHF.similarity_search(query)\n",
        "print(docs[0].page_content)\n",
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Ummaq-xJIS",
        "outputId": "e58b0979-b17d-41f6-f188-71f8741810dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good morning!\n",
            "Oh, hello!\n",
            "I want to report an accident\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Huggingface e5-large modell beágyazásokkal"
      ],
      "metadata": {
        "id": "ihYvWlYzAo0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embeddings_modelHF = HuggingFaceEmbeddings(model_name = 'intfloat/e5-large')\n",
        "\n",
        "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
        "raw_documents = TextLoader('dialog.txt').load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
        "documents = text_splitter.split_documents(raw_documents)\n",
        "dbHF2 = FAISS.from_documents(documents, embeddings_modelHF)"
      ],
      "metadata": {
        "id": "gDZX8Djj-fY0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "e156809d89304f1b88e9fb5f38272e6d",
            "d61e1cf580b745528f51cf45ab035a79",
            "92fb90d110854646bdf77822e1d2dc1b",
            "c7615a2f1bf54c38a0187f3d5bef371f",
            "973316e46c114a4082ce670c487ac498",
            "c9e7790db0fa476a9dd6de2c76ff3183",
            "6f884216d6a2409790305cc1144f2579",
            "e58debd4b2fe47018327013740c6798f",
            "56e6b969c86148fdafc5e17d69188f70",
            "4349757b96ee407ebedfd731a856e48e",
            "d6a590b8071d4b938980af36a5b6b0cf",
            "c28d29d2161140558a85ab37e7180f9b",
            "f52e65f7bdb74b05a49d7a60865c7c23",
            "976f36f94d3f49df9b8cb4c56f316a49",
            "763268ad86364745a354ad5c26db6e16",
            "ce78749d1fd14c28bb68f7520ba2ed47",
            "394791ff8c8245a8b0fd05aa92e4f185",
            "dac71f8b3c3b46058bdd56a96d397b68",
            "003765c374f94eab9c259dad8fdbee0a",
            "04746d1ea29549d4bede2479198990da",
            "87490cbf59e5433abdef0caa3d6d7f78",
            "7d8b3f2e96df44998cbe5a4a2ba922fc",
            "50b6c58b10e544c0b34ea46e97c7f19b",
            "235e23ec23084b9aba43e3d2186302aa",
            "1b02e1d7db2c4e7e8047d3416c426d6c",
            "e5efbd057547498dbbc49046ffcf0924",
            "6bfeedd6153b42cc8bca509b478a8ad4",
            "d085dff0f3c540068610144dd256911c",
            "57344ec2f0944f2d9f42af7331d9de5f",
            "236ba8a219ed43ad9d936e34481c5489",
            "79ca933cd9f24b55b637b15579ab61ea",
            "b5ac38f212a9454ead1df34daab95959",
            "56759f1c4b234052b371e9c0f3f62561",
            "a4ce6b45104b488f936f37023334ce41",
            "9f0abb7a67854b8699d5d7291cf12f6d",
            "09b0c0954cb646b9a5606efb15180d0b",
            "4174f0d863994daa98833afc7e3876da",
            "416c6189ade84fa19944ff43c881d0ff",
            "d825fd0329c94414b9c753f177beac4c",
            "b4987cda988545a283a145d006c0100e",
            "b1930ba6ff504564a6e1de7a4c951051",
            "09797fc6ba054bfeabd487a2df123bf4",
            "dd7d655f62b24e96afa286a8e82b9d9c",
            "8bb4c89137364571b5c505f455fa9107",
            "6acf289fb0864553aa5f8a6dc0b3c0fb",
            "121d0caedf2e434cbdea92baea97e666",
            "e817ccb341274477928d94f37a482be4",
            "a9d65d68837042f5b0b726b8e2d64928",
            "b91137c2c90044c9b9cc6a03a6e6dc38",
            "1798a70ce64c4713baaf8cbfcd7eb450",
            "818ec082ceb847699f10bc08fd09a9b4",
            "c1a3b0b8d24842d89657539a662372a1",
            "2e0bef36f9d24a3e8c85db32de137ad9",
            "c412890d161d4fcd9e12b6e19c20c186",
            "fde15d099c4d4f1cbacbc70a8b461b53",
            "12b86f19855c4ea79979b2852142b44c",
            "698dd2e32ef244db89a9847bc009a41e",
            "3ae3c79c1ddb4e809efad6809633ae92",
            "f18a69c65eeb4efa83ea4e213d3af0dd",
            "6c409346990c482b92bde3a074fdc973",
            "535eab750ad0454cb2ad85c87434c4d0",
            "364d2c6a033a4536a13dc78616a4f415",
            "20e7620f12154d5eb73a25a8a43ef241",
            "6813391bbe1c46339faedbe0a3d4e02e",
            "5cb212ba559e4538ab279ada3da0bbb6",
            "256471ce13834ca4926641c1ff3e339d",
            "721a231a64aa4a65b015e32dfe585845",
            "8761637cf03148bc80a3cda2c2052a20",
            "263f5cbc0adc49a2a432bd9b7b6efa89",
            "1ca84957413c458bac2ae361b5b1e09b",
            "3434c034afbc467eaf8f8cf03336e341",
            "11bb0ebae0d74bb8b6ea3c80b2911796",
            "0b422cb2f32a4dc38db5cde22f1ab64e",
            "bbb396dc21b543a0bf5aa0ab233fbdf4",
            "cddd1514d322483aa9d92574fbf80261",
            "554a31e2fe424d538b97563cca1f03e5",
            "9d0bb83df5354e12b959dc19e4b6664b",
            "f14b818e20e24ec59f0750d56c6ce8a9",
            "a37c92a085a34d948df6ccf83847b617",
            "002f2d8954744fe1b297bb4d5ae909c1",
            "5e23ec4272a94e80a81d21bcf917512d",
            "c7889d23e30f46349427315c2580da24",
            "90b85d557dd449d8a1f9f0099751ece8",
            "12fffa0ec2cb4b08b52d18dd33a93608",
            "6d55fb6660184703a4cad7ddb4d8b064",
            "619a02e91158475297f75c691bc91c01",
            "1ea827c18bd545659fc162f0b4b7dad5",
            "c75ada48f179440386675963832d6080",
            "1f4088d290354977939f961bfee9c7c3",
            "18d0860847c641888f37bd273fee8a0c",
            "744fea25ffec4d33ba135fa50adb4f84",
            "75a71a0a9d554c22831e21fe514fd637",
            "7e5772d855024b8c80c57dc18ac59269",
            "17a135d1ba394053accc2b27d6cad3f5",
            "8e11a3f22419473abc3e2ceb2141da93",
            "2449cd5008ea4d05a2caf4bac963121b",
            "b524a74fb9fb433498edfc945f51d3d8",
            "cb611b2e6ccb49c09dcad9fc027b5e8e",
            "af2afaa9e3514348aabfa9e3b2d04d65",
            "f862024b5c204861b40314fca77ad1c7",
            "9d9ebaab384e45d9bc2f7aaf61d45310",
            "eafc2e73d420433e9d99c6b5cace9f24",
            "645209f3702b4ed19994ea3e780eef81",
            "c48931c2f41d446bafd305ba8245670b",
            "52d0c13cf44348ea9923015b14c005bf",
            "c553d002018c42d9be7dfbb2e1c3ac1d",
            "941dca28cbab4eb4b91e27d88f547e33",
            "ca8f1c77f15b4e779e11f498e8e01a80",
            "7076616089b74252bb6f39b5c78c4157",
            "83b1a8504367423793f600c96eb29b9e"
          ]
        },
        "outputId": "e0d78802-aaac-468e-cdae-3eb99eab8675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e156809d89304f1b88e9fb5f38272e6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/67.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c28d29d2161140558a85ab37e7180f9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50b6c58b10e544c0b34ea46e97c7f19b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4ce6b45104b488f936f37023334ce41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6acf289fb0864553aa5f8a6dc0b3c0fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12b86f19855c4ea79979b2852142b44c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "721a231a64aa4a65b015e32dfe585845"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f14b818e20e24ec59f0750d56c6ce8a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f4088d290354977939f961bfee9c7c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f862024b5c204861b40314fca77ad1c7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the reason for calling?\"\n",
        "docs = dbHF2.similarity_search(query)\n",
        "print(docs[0].page_content)\n",
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4d2040-aed6-4224-e04c-c24b066f7061",
        "id": "Ryr6CAJ4-xS_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry to hear that. May I ask your name?\n",
            "I want to report an accident\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lekérdezők (Retrievers).\n",
        "A LangChainben a retriever olyan komponens, amely képes egy strukturálatlan lekérdezésre releváns dokumentumokat visszaadni, például egy természetes nyelvi kérdésre vagy kulcsszóra. A retrievernek nem kell magának tárolnia a dokumentumokat, csak egy forrásból kell visszakeresnie őket. A retriever különböző módszereket használhat a releváns dokumentumok megtalálására, például kulcsszóegyeztetést, szemantikai keresést vagy rangsorolási algoritmusokat.\n",
        "\n",
        "A retriever és a vektortároló (vector store) közötti különbség az, hogy a retriever általánosabb és rugalmasabb. A retriever bármilyen módszerrel megtalálhatja a releváns dokumentumokat, míg a vektortároló beágyazásokra és hasonlósági metrikákra támaszkodik. A retriever különböző dokumentumforrásokat is használhat, például weboldalakat, adatbázisokat vagy fájlokat, míg a vektortárolónak magának kell tárolnia az adatokat.\n",
        "\n",
        "Azonban a vektortároló használható retriever alapjaként is, ha az adatokat beágyazza és indexeli egy vektortároló. Ebben az esetben a retriever a vektortárolót használhatja hasonlósági keresés elvégzésére a beágyazott adatokon, és visszaadhatja a legrelevánsabb dokumentumokat.  Ez az egyik fő típusa a LangChainben használt retrievereknek, és vektoros tárolón alapuló retrievernek (vector store retriever) hívjuk.\n",
        "\n",
        "Például vegyük a korábban inicializált FAISS vektortárolót, és \"szereljünk fel\" rá egy retrievert."
      ],
      "metadata": {
        "id": "luN9NYtgBUaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OpenAi beágyazásokat használva"
      ],
      "metadata": {
        "id": "wj9hCh6JB_JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "retriever = db.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(api_key = userdata.get('openai_api_key')), chain_type=\"stuff\", retriever=retriever)\n",
        "query = \"What was the reason of the call?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ZsdXIp8gy_HE",
        "outputId": "14a220c6-6804-426a-8610-d54c0cab86d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The reason for the call was to report an accident.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ingyenes Huggingface beágyazásokat használva"
      ],
      "metadata": {
        "id": "yd-udqL8CJki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "retriever = dbHF2.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(api_key = userdata.get('openai_api_key')), chain_type=\"stuff\", retriever=retriever)\n",
        "query = \"What was the reason of the call?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cOsri_jUBy2z",
        "outputId": "80fb027d-b07c-4b4f-c725-e5d4a53f5ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The reason for the call was to report an accident. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"What was the reason of the call?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hDWaUnxk74W",
        "outputId": "718f813a-0a47-4879-aef1-4a57f5786466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Sorry to hear that. May I ask your name?', metadata={'source': 'dialog.txt'}),\n",
              " Document(page_content='I want to report an accident', metadata={'source': 'dialog.txt'}),\n",
              " Document(page_content='Good morning!\\nOh, hello!', metadata={'source': 'dialog.txt'}),\n",
              " Document(page_content='Sure, Mario Rossi.', metadata={'source': 'dialog.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" #\"tiiuae/falcon-7b-instruct\"\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.7, \"max_length\": 1000}, huggingfacehub_api_token=userdata.get('HF_TOKEN')\n",
        ")\n",
        "print(llm(\"what was the first disney movie?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX7waSpACUHC",
        "outputId": "144dfc5c-ee3e-400e-d565-cb6101a402e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what was the first disney movie?\n",
            "\n",
            "The first Disney movie was actually a series of short cartoons called \"Steamboat Willie.\" It was released on November 18, 1928, and it featured Mickey Mouse for the very first time. This was a significant breakthrough for animation because Steamboat Willie was the first cartoon to synchronize sound with the movement of the characters' lips, a technique known as synchronized sound or \"talking\" cartoons. This innovation revolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "retriever = dbHF2.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "query = \"What was the reason of the call?\"\n",
        "answer = qa.run(query)"
      ],
      "metadata": {
        "id": "gKUto-KsDQap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "sgovJPM-C4BI",
        "outputId": "cf66b7ae-2ce8-47a5-b52c-88fad67892a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nSorry to hear that. May I ask your name?\\n\\nI want to report an accident\\n\\nGood morning!\\nOh, hello!\\n\\nSure, Mario Rossi.\\n\\nQuestion: What was the reason of the call?\\nHelpful Answer: The caller identified himself as Mario Rossi and stated that he wanted to report an accident.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Láncok (Chains)\n",
        "A láncok előre meghatározott műveleti és LLM-hívási sorozatok, amelyek megkönnyítik az összetett alkalmazások létrehozását. Ehhez szükség lehet az LLM-ek egymással vagy más komponensekkel való kombinálására. A LangChain négy fő lánctípust kínál, hogy könnyen elkezdhesd a munkát:\n",
        "\n",
        "LLMChain. Ez a leggyakoribb lánctípus. Egy PromptTemplate-et (prompt sablon), egy LLM-et és egy opcionális output parser-t (kimeneti elemző) tartalmaz.\n",
        "\n",
        "Definíció\n",
        "A kimeneti elemző (output parser) egy olyan komponens, amely segít a nyelvi modell válaszait strukturálni. Ez egy olyan osztály, amely két fő metódust valósít meg: get_format_instructions és parse. A get_format_instructions metódus egy olyan karakterláncot ad vissza, amely utasításokat tartalmaz arra vonatkozóan, hogyan kell formázni a nyelvi modell kimenetét. A parse metódus egy karakterláncot vesz be (amely feltételezhetően egy nyelvi modell válasza), és valamilyen struktúrává elemzi, például szótárrá, listává vagy egyéni objektummá.\n",
        "\n",
        "Ez a lánc több bemeneti változót fogad, a PromptTemplate segítségével prompt-tá formázza őket, átadja a modellnek, majd az OutputParser-t használja (ha meg van adva), hogy a LLM kimenetét végleges formátumba elemezze. Például vegyük elő azt a prompt sablont, amit az előző részben készítettünk.\n",
        "\n"
      ],
      "metadata": {
        "id": "rQnvapUhGdDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "template = \"\"\"Sentence: {sentence}\n",
        "Translation in {language}:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])"
      ],
      "metadata": {
        "id": "XVo9oS7TEi0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OpenAi\n"
      ],
      "metadata": {
        "id": "So8R6rjdG1aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMChain\n",
        "llm = OpenAI(api_key = userdata.get('openai_api_key'), temperature=0)\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "llm_chain.predict(sentence=\"the cat is on the table\", language=\"magyar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gNN3wHo7E7YW",
        "outputId": "353c650d-43d1-4fc5-8be9-5cba063a8fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' a macska az asztalon van'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Huggingface Mixtral modell"
      ],
      "metadata": {
        "id": "atqUgtOhG5rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMChain\n",
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.7, \"max_length\": 1000}, huggingfacehub_api_token=userdata.get('HF_TOKEN')\n",
        ")\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "answer= llm_chain.predict(sentence=\"the cat is on the table\", language=\"magyar\")"
      ],
      "metadata": {
        "id": "654H8oBnFGC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "6h-VCBkLUYrD",
        "outputId": "d802e012-d596-4cd5-c090-70becc1ab2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentence: the cat is on the table\\nTranslation in magyar: a macska az asztalon van\\n\\nWhy is the cat on the table?\\nWhy is it always the cat, and not the dog?\\nWhy a table? Why not a chair or an armchair?\\nDoes the cat have business on the table?\\nHow did the cat get on the table?\\n\\nThese are questions that arose in my mind when I saw the sentence on an exercise.\\n\\nThe cat is on the table.\\n\\nI can imagine'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SequentialChain (Szekvenciális lánc).\n",
        "Ez a lánctípus lehetővé teszi, hogy több láncot futtass sorrendben. Megadhatod a láncok sorrendjét és azt is, hogyan adják át a kimeneteiket a következő láncnak. A szekvenciális lánc legegyszerűbb modulja alapértelmezetten az egyik lánc kimenetét használja a következő lánc bemeneteként. Azonban használhatsz egy összetettebb modult is, hogy rugalmasabban állíthasd be a láncok közötti bemeneteket és kimeneteket."
      ],
      "metadata": {
        "id": "XmGELU1UROGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "# This is an LLMChain to write a synopsis given a title of a play.\n",
        "llm = OpenAI(api_key = userdata.get('openai_api_key'), temperature=.7)\n",
        "template = \"\"\"You are a comedian. Generate a joke on the following {topic}\n",
        "Joke:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template, output_key='out1')\n",
        "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "template = \"\"\"You are translator. Given a text {out1}, translate it to hungarian\n",
        "Translation:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"out1\"], template=template)\n",
        "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "GUAmKewAHm9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
        "translated_joke = overall_chain.run(\"Cats and Dogs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djSs6hzkIClz",
        "outputId": "7eb77a9f-822c-4a05-8b5f-ad578699e439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m Why did the cat and dog decide to open a bakery together?\n",
            "\n",
            "Because they wanted to make paw-some treats!\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m Miért döntöttek úgy a macska és kutya, hogy együtt nyitnak egy pékséget?\n",
            "\n",
            "Mert szerettek volna csodás finomságokat készíteni!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_joke"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sAcpo-TNVzDY",
        "outputId": "0490de73-ce68-4fdf-cbc8-d0e5e39977c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Miért mentek együtt a macska és a kutya terápiára?\\nMert belefáradtak a saját farkuk üldözésébe és szükségük volt valakire, aki segít nekik megtalálni belső békéjüket!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mixtral modell"
      ],
      "metadata": {
        "id": "c7g5E_WqVl2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMChain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.7, \"max_length\": 1000}, huggingfacehub_api_token=userdata.get('HF_TOKEN')\n",
        ")\n",
        "\n",
        "\n",
        "template = \"\"\"You are a comedian. Generate a joke on the following {topic}\n",
        "Joke:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
        "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "template = \"\"\"You are translator. Given a text {input}, translate it to hungarian, no need to explanation\n",
        "Translation:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"input\"], template=template)\n",
        "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "ruysQTYNUtAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
        "translated_joke = overall_chain.run(\"Cats and Dogs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4d87c8e-4d10-44f1-f2a3-bd6dd5e26ddd",
        "id": "oWMMo8BGUtAp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mYou are a comedian. Generate a joke on the following Cats and Dogs\n",
            "Joke:\n",
            "\n",
            "There once was a cat who thought he was a dog.\n",
            "He would chase his tail, bark at the mailman, and even try to fetch the newspaper.\n",
            "One day, his owner took him to the vet and said, \"Doc, I think there's something wrong with my cat. He thinks he's a dog.\"\n",
            "The vet looked at the cat and said, \"Well, he may think he's a dog, but I can assure you he'\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mYou are translator. Given a text You are a comedian. Generate a joke on the following Cats and Dogs\n",
            "Joke:\n",
            "\n",
            "There once was a cat who thought he was a dog.\n",
            "He would chase his tail, bark at the mailman, and even try to fetch the newspaper.\n",
            "One day, his owner took him to the vet and said, \"Doc, I think there's something wrong with my cat. He thinks he's a dog.\"\n",
            "The vet looked at the cat and said, \"Well, he may think he's a dog, but I can assure you he', translate it to hungarian, no need to explanation\n",
            "Translation:\n",
            "Volt egyszer egy macska, aki úgy gondolta magáról, hogy kutya.\n",
            "Ő a farkát kergette, a postást ugatta ki, és még újságot is hozni próbált.\n",
            "Egy napon a tulajdonosa elvitte a vételre és azt mondta: \"Doktor, a macskámnak valami ninc\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_joke"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "gdwY46S-VZnH",
        "outputId": "b37cb250-d6a4-49e8-c44c-07b6c834f7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are translator. Given a text You are a comedian. Generate a joke on the following Cats and Dogs\\nJoke:\\n\\nThere once was a cat who thought he was a dog.\\nHe would chase his tail, bark at the mailman, and even try to fetch the newspaper.\\nOne day, his owner took him to the vet and said, \"Doc, I think there\\'s something wrong with my cat. He thinks he\\'s a dog.\"\\nThe vet looked at the cat and said, \"Well, he may think he\\'s a dog, but I can assure you he\\', translate it to hungarian, no need to explanation\\nTranslation:\\nVolt egyszer egy macska, aki úgy gondolta magáról, hogy kutya.\\nŐ a farkát kergette, a postást ugatta ki, és még újságot is hozni próbált.\\nEgy napon a tulajdonosa elvitte a vételre és azt mondta: \"Doktor, a macskámnak valami ninc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feladat\n",
        "Az alábbi szövegfájlt töltsed le és készíts hozzá egy vektor adatbázist, majd tedd fel az alábbi kérdéseket:\n",
        "* \"Ki volt Nemecsek?\"\n",
        "* \"Mi a gittegylet szerepe?\"\n",
        "* \"Ki volt a vörösingesek vezetője?\"\n",
        "\n",
        "Nézzd meg a retriver által adott válaszokat is!\n"
      ],
      "metadata": {
        "id": "NodKUiRVwz4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1op1icuNvSOmXzfrfFPfGGsi-BX4T6rNI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwOB-iVmcwWj",
        "outputId": "f098f04e-44c3-44c8-d9b0-6ed18f0571f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1op1icuNvSOmXzfrfFPfGGsi-BX4T6rNI\n",
            "To: /content/PUF.txt\n",
            "\r  0% 0.00/295k [00:00<?, ?B/s]\r100% 295k/295k [00:00<00:00, 71.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wmvmC6Ro650t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBLUXeh1653p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11Otp-PT657h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_GbUD_j659s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erGcOh7S66AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9ot87ci66CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feldadat\n",
        "Készíts langchain segítségével egy láncot amely egy adott témával {topic} kapcsolatosan vissza adj a téma 4 legfontosabb területét,\n",
        "majd a lánc második eleme ezt pontonként kifejti részletesen."
      ],
      "metadata": {
        "id": "U5HS5RntxcYG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h8cRB0og72NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZ1tB6FZ72W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qeg_0Ibu72Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUaAUkHr72cg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}